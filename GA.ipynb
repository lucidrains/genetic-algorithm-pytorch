{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm\n",
    "\n",
    "In this Jupyter notebook we will explore a very simple GA. Lets start by loading the breast cancer dataset from the UCI repo (https://archive.ics.uci.edu/ml/datasets/breast+cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cancer=load_breast_cancer()\n",
    "df = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])\n",
    "label=cancer[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets split this data into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the model into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, label, test_size=0.30, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "logmodel = LogisticRegression() # build the logistic regression machine\n",
    "logmodel.fit(X_train,y_train) # the training function\n",
    "predictions = logmodel.predict(X_test) # run on the test data\n",
    "print(\"Accuracy = \"+ str(accuracy_score(y_test,predictions))) # what is the accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOW, what if we use a genetic algorithm for feature selection?** \n",
    "\n",
    "That is, maybe all the features are not to our benefit. What if instead we use a subset? Well, how can we find this subset? Why not use a GA? Well, its an example for this class, not arguing here that a GA is the best solution to this particular problem. Trying to keep our example small enough for you to follow!\n",
    "\n",
    "Howabout a chromosome is a {USE, NOT USE} per feature. \n",
    "\n",
    "First, lets initilize the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initilization_of_population(size, n_feat):\n",
    "    population = []\n",
    "    for i in range(size): # do for each chromo\n",
    "        chromosome = np.ones(n_feat,dtype=bool) # include all features to start\n",
    "        chromosome[:int(0.3*n_feat)]=False # now, turn off 30% of our features (yes, you could have passed this as an argument!!!)\n",
    "        np.random.shuffle(chromosome) # randomly shuffle who is on/off\n",
    "        population.append(chromosome) # add to our population\n",
    "    return population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets define our fitness function. \n",
    "\n",
    "For each chromosome, we need to train with respect to its feature subset then evaluate it. Yes, that requires a bit of computation.\n",
    "\n",
    "The function returns a sorted (best to worst) set of scores and chromosomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_score(population):\n",
    "    scores = []\n",
    "    for chromosome in population: # lets walk through each chromo\n",
    "        logmodel.fit(X_train.iloc[:,chromosome],y_train) # use the features we selected to train a model\n",
    "        predictions = logmodel.predict(X_test.iloc[:,chromosome]) # eval that model\n",
    "        scores.append(accuracy_score(y_test,predictions)) # store that score\n",
    "    scores, pop = np.array(scores), np.array(population.copy()) \n",
    "    inds = np.argsort(scores) # returns the indices that would sort an array\n",
    "    scores = list(scores[inds][::-1]) # make it a list data structure\n",
    "    pop = list(pop[inds,:][::-1]) # make it a list data structure\n",
    "    return scores, pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, perform selection, using roulette wheel. In particular, lets say there are two parents to make a child. I will generate twice the number of chromosomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection(scores,pop_after_fit):\n",
    "    \"\"\"select pops by their scores using roulette wheel method\"\"\"\n",
    "    population_nextgen = []\n",
    "    # first two entries will be used for elitism (i.e., keep that best solution from generation to next)\n",
    "    population_nextgen.append(pop_after_fit[0].copy())\n",
    "    population_nextgen.append(pop_after_fit[0].copy()) # storing the same entries twice?population_nextgen.append(pop_after_fit[0].copy()) # storing the same entries twice?\n",
    "    # now, select the other parents\n",
    "    max_val = np.sum(scores)\n",
    "    for i in range(1,len(scores)):\n",
    "        for k in range(2): # 2 parents \n",
    "            pick = random.uniform(0, max_val)\n",
    "            current = 0\n",
    "            for j in range(len(scores)): # accum\n",
    "                current += scores[j]\n",
    "                if current > pick: # our exit criteria based on the probability above\n",
    "                    break \n",
    "            population_nextgen.append(pop_after_fit[j].copy()) \n",
    "    return population_nextgen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crossover here takes two parents and makes a child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossover(pop_after_sel,sz):\n",
    "    # corssover is not random, it is based on the order of the parents 2i and 2i+1\n",
    "    # how many features?\n",
    "    n_feat = len(pop_after_sel[0])\n",
    "    # next generation\n",
    "    population_nextgen = [] \n",
    "    # first, elitism\n",
    "    population_nextgen.append(pop_after_sel[0].copy())\n",
    "    # now, handle the rest\n",
    "    for i in range(1,sz):\n",
    "        # make our child (parent 1)\n",
    "        child = pop_after_sel[2*i+0].copy()\n",
    "        # get the second parent\n",
    "        parent2 = pop_after_sel[2*i+1].copy()\n",
    "        # two point crossover\n",
    "        first_cross_point = random.randint(0,n_feat)\n",
    "        second_cross_point = random.randint(0,n_feat)\n",
    "        # did we get the same point? have to deal with that\n",
    "        if( first_cross_point == second_cross_point ):\n",
    "            first_cross_point = 0\n",
    "            second_cross_point = n_feat\n",
    "        # are our swap indices not in order? have to deal with that\n",
    "        if( first_cross_point > second_cross_point ):\n",
    "            swaper = first_cross_point\n",
    "            first_cross_point = second_cross_point\n",
    "            second_cross_point = swaper\n",
    "        # swap \n",
    "        child[first_cross_point:second_cross_point] = parent2[first_cross_point:second_cross_point]\n",
    "        population_nextgen.append(child) \n",
    "    return population_nextgen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutation time!!!\n",
    "\n",
    "Like all the operations above, there are many ways to implement them (which we talk about in the class). Now, with respect to mutation, here we go through each chromosome and mutate using the probability that the user specified, per feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutation(pop_after_cross,mutation_rate,sz):\n",
    "    population_nextgen = []\n",
    "    population_nextgen.append(pop_after_cross[0].copy())\n",
    "    for i in range(1,sz): # go through each individual\n",
    "        chromosome = pop_after_cross[i].copy()\n",
    "        for j in range(len(chromosome)): # go through each feature\n",
    "            if random.random() < mutation_rate:\n",
    "                chromosome[j]= not chromosome[j] # this is the mutation\n",
    "        population_nextgen.append(chromosome.copy())\n",
    "    return population_nextgen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main function/loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def generations(sz,n_feat,mutation_rate,n_gen,X_train,\n",
    "                                   X_test, y_train, y_test):\n",
    "    # size = how many chromo's\n",
    "    # n_feat = how many features\n",
    "    # mutation rate, [0,1]\n",
    "    # number of generations to run\n",
    "    # the rest is our training and testing data and labels\n",
    "    best_chromo = []\n",
    "    best_score = []\n",
    "    population_nextgen = initilization_of_population(sz,n_feat)\n",
    "    stats_min = np.zeros(n_gen)\n",
    "    stats_avg = np.zeros(n_gen)\n",
    "    stats_max = np.zeros(n_gen)\n",
    "    for i in tqdm(range(n_gen)):\n",
    "        # evaluate our chromos\n",
    "        scores, pop_after_fit = fitness_score(population_nextgen)\n",
    "        # keep track of best chromo and its score (what we return the end of the day)\n",
    "        best_chromo.append(pop_after_fit[0].copy())\n",
    "        best_score.append(scores[0].copy())        \n",
    "        # record some stats\n",
    "        stats_min[i] = np.min(scores)\n",
    "        stats_max[i] = np.amax(scores)\n",
    "        stats_avg[i] = np.mean(scores)\n",
    "        # do our GA\n",
    "        pop_sel = selection(scores,pop_after_fit)\n",
    "        pop_after_cross = crossover(pop_sel,sz)\n",
    "        population_nextgen = mutation(pop_after_cross,mutation_rate,sz)\n",
    "    return best_chromo,best_score,stats_min,stats_avg,stats_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm and plot some statistics per generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 100 # number of individuals\n",
    "feat = 30 # number of features in our problem\n",
    "ngen = 100 # number of generations\n",
    "chromo,score,stats_min,stats_avg,stats_max=generations(sz=s,n_feat=feat,mutation_rate=0.10,n_gen=ngen,X_train=X_train,X_test=X_test,y_train=y_train,y_test=y_test)\n",
    "\n",
    "# plot the statistics\n",
    "plt.plot(stats_min,'r')\n",
    "plt.plot(stats_avg,'b')\n",
    "plt.plot(stats_max,'g')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('generations')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
